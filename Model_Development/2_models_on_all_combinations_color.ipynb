{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gUfvZh_xIPA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io\n",
        "import scipy.stats\n",
        "from scipy.optimize import curve_fit\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.impute import SimpleImputer\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "NUM_ITERATIONS = 500\n",
        "N_JOBS = -1\n",
        "\n",
        "def get_dataset_info(prompt_text):\n",
        "    print(f\"\\n--- {prompt_text} ---\")\n",
        "    print(\"1. UID (Expects 'mos_UID.xlsx')\")\n",
        "    print(\"2. SAUD (Expects 'SAUD_MOS.xlsx')\")\n",
        "\n",
        "    choice = input(\"Enter 1 or 2: \").strip()\n",
        "\n",
        "    if choice == '1':\n",
        "        mos_file = \"mos_UID.xlsx\"\n",
        "        name = \"UID\"\n",
        "    elif choice == '2':\n",
        "        mos_file = \"SAUD_MOS.xlsx\"\n",
        "        name = \"SAUD\"\n",
        "    else:\n",
        "        print(\"Invalid selection. Defaulting to UID.\")\n",
        "        mos_file = \"mos_UID.xlsx\"\n",
        "        name = \"UID\"\n",
        "\n",
        "    if not os.path.exists(mos_file):\n",
        "        print(f\"[ERROR] '{mos_file}' not found. Please upload it.\")\n",
        "        return None, None, None\n",
        "\n",
        "    zip_name = input(f\"Enter the features ZIP file for {name}: \").strip()\n",
        "\n",
        "    if not os.path.exists(zip_name):\n",
        "        print(f\"[ERROR] '{zip_name}' not found.\")\n",
        "        return None, None, None\n",
        "\n",
        "    return name, mos_file, zip_name\n",
        "\n",
        "def unzip_features(zip_name, extract_to):\n",
        "    # Clean up if exists\n",
        "    if os.path.exists(extract_to):\n",
        "        import shutil\n",
        "        shutil.rmtree(extract_to)\n",
        "    os.makedirs(extract_to)\n",
        "\n",
        "    print(f\"Unzipping {zip_name} into {extract_to}...\")\n",
        "    with zipfile.ZipFile(zip_name, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "def load_features_from_folder(feature_folder, df_mos, name_col, mos_col):\n",
        "    \"\"\"\n",
        "    Loads features from a specific folder and matches with MOS.\n",
        "    Uses os.walk to find .mat files recursively, handling mirrored directory structures.\n",
        "    \"\"\"\n",
        "    # 1. Map Files\n",
        "    file_map = {}\n",
        "    for root, _, files in os.walk(feature_folder):\n",
        "        for f in files:\n",
        "            if f.endswith('.mat'):\n",
        "                # Store lower case keys for robust matching\n",
        "                file_map[f.lower()] = os.path.join(root, f)\n",
        "                file_map[os.path.splitext(f)[0].lower()] = os.path.join(root, f)\n",
        "\n",
        "    features = []\n",
        "    scores = []\n",
        "\n",
        "    # 2. Align\n",
        "    for _, row in df_mos.iterrows():\n",
        "        fname = str(row[name_col]).strip()\n",
        "        key = os.path.basename(fname).lower()\n",
        "        key_no_ext = os.path.splitext(key)[0]\n",
        "\n",
        "        path = file_map.get(key) or file_map.get(key_no_ext)\n",
        "\n",
        "        if path:\n",
        "            try:\n",
        "                mat = scipy.io.loadmat(path)\n",
        "                # Find the feature variable (ignore __header__, etc)\n",
        "                k = [k for k in mat.keys() if not k.startswith('__')][0]\n",
        "                feat_vec = np.array(mat[k]).flatten()\n",
        "                features.append(feat_vec)\n",
        "                scores.append(row[mos_col])\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    return np.array(features), np.array(scores)\n",
        "\n",
        "def logistic_func(X, b1, b2, b3, b4):\n",
        "    logisticPart = 1 + np.exp(-(X - b3) / np.abs(b4))\n",
        "    yhat = b2 + (b1 - b2) / logisticPart\n",
        "    return yhat\n",
        "\n",
        "def compute_metrics(y_pred, y):\n",
        "    try:\n",
        "        beta_init = [np.max(y), np.min(y), np.mean(y_pred), 0.5]\n",
        "        popt, _ = curve_fit(logistic_func, y_pred, y, p0=beta_init, maxfev=int(1e8))\n",
        "        y_pred_logistic = logistic_func(y_pred, *popt)\n",
        "    except:\n",
        "        y_pred_logistic = y_pred\n",
        "\n",
        "    SRCC = scipy.stats.spearmanr(y, y_pred)[0]\n",
        "    try:\n",
        "        KRCC = scipy.stats.kendalltau(y, y_pred)[0]\n",
        "    except:\n",
        "        KRCC = scipy.stats.kendalltau(y, y_pred, method='asymptotic')[0]\n",
        "\n",
        "    PLCC = scipy.stats.pearsonr(y, y_pred_logistic)[0]\n",
        "    RMSE = np.sqrt(mean_squared_error(y, y_pred_logistic))\n",
        "    return [SRCC, KRCC, PLCC, RMSE]\n",
        "\n",
        "def run_evaluation(iter_idx, X_train_full, y_train_full, X_test_full, y_test_full, is_same_dataset):\n",
        "    np.random.seed(iter_idx)\n",
        "\n",
        "    # CASE A: Same Dataset (70/10/20 Split)\n",
        "    if is_same_dataset:\n",
        "        n = len(y_train_full)\n",
        "        perm = np.random.permutation(n)\n",
        "        n_train = int(n * 0.7)\n",
        "        n_val = int(n * 0.1)\n",
        "\n",
        "        train_idx = perm[:n_train]\n",
        "        val_idx = perm[n_train:n_train+n_val]\n",
        "        test_idx = perm[n_train+n_val:]\n",
        "\n",
        "        X_tr, y_tr = X_train_full[train_idx], y_train_full[train_idx]\n",
        "        X_val, y_val = X_train_full[val_idx], y_train_full[val_idx]\n",
        "        X_te, y_te = X_train_full[test_idx], y_train_full[test_idx]\n",
        "\n",
        "    # CASE B: Cross Dataset (Train on A, Test on B)\n",
        "    else:\n",
        "        # Use 90% of Train Set for Training, 10% for Hyperparam Validation\n",
        "        n = len(y_train_full)\n",
        "        perm = np.random.permutation(n)\n",
        "        n_train = int(n * 0.9)\n",
        "\n",
        "        train_idx = perm[:n_train]\n",
        "        val_idx = perm[n_train:]\n",
        "\n",
        "        X_tr, y_tr = X_train_full[train_idx], y_train_full[train_idx]\n",
        "        X_val, y_val = X_train_full[val_idx], y_train_full[val_idx]\n",
        "        X_te, y_te = X_test_full, y_test_full # Test on full Dataset B\n",
        "\n",
        "    # Scale\n",
        "    scaler = MinMaxScaler((-1, 1))\n",
        "    X_tr = scaler.fit_transform(X_tr)\n",
        "    X_val = scaler.transform(X_val)\n",
        "    X_te = scaler.transform(X_te)\n",
        "\n",
        "    # Grid Search\n",
        "    best_srcc = -1\n",
        "    best_p = {'C': 10, 'gamma': 0.1}\n",
        "\n",
        "    # Small Grid\n",
        "    for c in [1, 10, 100]:\n",
        "        for g in [0.01, 0.1, 1]:\n",
        "            m = SVR(C=c, gamma=g)\n",
        "            m.fit(X_tr, y_tr)\n",
        "            p = m.predict(X_val)\n",
        "            s = scipy.stats.spearmanr(y_val, p)[0]\n",
        "            if s > best_srcc:\n",
        "                best_srcc = s\n",
        "                best_p = {'C': c, 'gamma': g}\n",
        "\n",
        "    # Final Train\n",
        "    X_final = np.vstack((X_tr, X_val))\n",
        "    y_final = np.concatenate((y_tr, y_val))\n",
        "    final_m = SVR(C=best_p['C'], gamma=best_p['gamma'])\n",
        "    final_m.fit(X_final, y_final)\n",
        "\n",
        "    # Test\n",
        "    preds = final_m.predict(X_te)\n",
        "    return compute_metrics(preds, y_te)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=== Feature Combination Analysis (Supports H/S/V/HS Modes) ===\")\n",
        "\n",
        "    # 1. Inputs\n",
        "    t_name, t_mos_file, t_zip = get_dataset_info(\"SELECT TRAINING DATASET\")\n",
        "    if not t_name: exit()\n",
        "\n",
        "    test_name, test_mos_file, test_zip = get_dataset_info(\"SELECT TESTING DATASET\")\n",
        "    if not test_name: exit()\n",
        "\n",
        "    is_same = (t_name == test_name) and (t_zip == test_zip)\n",
        "    if is_same:\n",
        "        print(f\"\\n[Mode] Same Dataset Evaluation (70/10/20 on {t_name})\")\n",
        "    else:\n",
        "        print(f\"\\n[Mode] Cross Dataset Evaluation (Train {t_name} -> Test {test_name})\")\n",
        "\n",
        "    # 2. Unzip\n",
        "    train_extract_path = \"./train_feats_extracted\"\n",
        "    test_extract_path = \"./test_feats_extracted\"\n",
        "\n",
        "    unzip_features(t_zip, train_extract_path)\n",
        "    if not is_same:\n",
        "        unzip_features(test_zip, test_extract_path)\n",
        "    else:\n",
        "        test_extract_path = train_extract_path\n",
        "\n",
        "    # 3. Load MOS Dataframes\n",
        "    df_train = pd.read_excel(t_mos_file)\n",
        "    df_train.columns = [c.strip() for c in df_train.columns]\n",
        "\n",
        "    df_test = pd.read_excel(test_mos_file)\n",
        "    df_test.columns = [c.strip() for c in df_test.columns]\n",
        "\n",
        "    # Detect MOS Columns\n",
        "    def get_cols(df):\n",
        "        if 'Image' in df.columns: name = 'Image'\n",
        "        elif 'image_name' in df.columns: name = 'image_name'\n",
        "        else: name = df.columns[0]\n",
        "        return name, 'MOS'\n",
        "\n",
        "    t_name_col, t_mos_col = get_cols(df_train)\n",
        "    test_name_col, test_mos_col = get_cols(df_test)\n",
        "\n",
        "    # 4. Find Feature Subfolders (Handing New Mode Structure)\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Handle case where zip creates a single wrapper folder\n",
        "    root_items = [f.path for f in os.scandir(train_extract_path) if f.is_dir()]\n",
        "    if len(root_items) == 1:\n",
        "        # Check if this single folder is NOT a Mode or Config (e.g. 'Extracted_Features')\n",
        "        base = os.path.basename(root_items[0])\n",
        "        if base not in ['H', 'S', 'V', 'HS'] and not base.startswith('S1'):\n",
        "             train_extract_path = root_items[0]\n",
        "             if not is_same:\n",
        "                 # Align test path\n",
        "                 test_items = [f.path for f in os.scandir(test_extract_path) if f.is_dir()]\n",
        "                 if len(test_items) == 1:\n",
        "                     test_extract_path = test_items[0]\n",
        "\n",
        "    # Detect Structure: Modes (H/S/V/HS) vs Flat Configs (S1_FA...)\n",
        "    potential_modes = ['H', 'S', 'V', 'HS']\n",
        "    items = [f.name for f in os.scandir(train_extract_path) if f.is_dir()]\n",
        "\n",
        "    # Check if we see the Color Mode folders\n",
        "    has_modes = any(m in items for m in potential_modes)\n",
        "\n",
        "    experiments = [] # List of dicts {label, train_path, test_path}\n",
        "\n",
        "    if has_modes:\n",
        "        print(f\"Detected Color Mode Structure (H, S, V, HS)...\")\n",
        "        for mode in items:\n",
        "            mode_path = os.path.join(train_extract_path, mode)\n",
        "\n",
        "            # Find configs inside the mode folder (S1_FA, S12_FAB...)\n",
        "            configs = [f.name for f in os.scandir(mode_path) if f.is_dir()]\n",
        "\n",
        "            for cfg in configs:\n",
        "                experiments.append({\n",
        "                    'label': f\"{mode} - {cfg}\", # Example: H - S1_FAB\n",
        "                    'train_path': os.path.join(train_extract_path, mode, cfg),\n",
        "                    'test_path': os.path.join(test_extract_path, mode, cfg) if test_extract_path else None\n",
        "                })\n",
        "    else:\n",
        "        print(f\"Detected Flat Configuration Structure...\")\n",
        "        for cfg in items:\n",
        "            experiments.append({\n",
        "                'label': cfg,\n",
        "                'train_path': os.path.join(train_extract_path, cfg),\n",
        "                'test_path': os.path.join(test_extract_path, cfg) if test_extract_path else None\n",
        "            })\n",
        "\n",
        "    if not experiments:\n",
        "        print(\"Error: No feature subfolders found in zip.\")\n",
        "        exit()\n",
        "\n",
        "    # Sort for cleaner output\n",
        "    experiments.sort(key=lambda x: x['label'])\n",
        "\n",
        "    # 5. Iterate Over All Feature Sets\n",
        "    results_table = []\n",
        "    print(f\"\\nEvaluating {len(experiments)} feature sets...\")\n",
        "\n",
        "    for exp in tqdm(experiments, desc=\"Evaluations\"):\n",
        "        config_label = exp['label']\n",
        "        t_folder = exp['train_path']\n",
        "        test_folder = exp['test_path']\n",
        "\n",
        "        # Load Data\n",
        "        X_train, y_train = load_features_from_folder(t_folder, df_train, t_name_col, t_mos_col)\n",
        "\n",
        "        if is_same:\n",
        "            X_test, y_test = X_train, y_train\n",
        "        else:\n",
        "            # Check if test folder exists (it should if zips match)\n",
        "            if not test_folder or not os.path.exists(test_folder):\n",
        "                continue\n",
        "            X_test, y_test = load_features_from_folder(test_folder, df_test, test_name_col, test_mos_col)\n",
        "\n",
        "        if len(X_train) == 0 or len(X_test) == 0:\n",
        "            continue\n",
        "\n",
        "        # Impute\n",
        "        if np.isnan(X_train).any():\n",
        "            imp = SimpleImputer(strategy='mean')\n",
        "            X_train = imp.fit_transform(X_train)\n",
        "            if is_same:\n",
        "                X_test = X_train\n",
        "            else:\n",
        "                if np.isnan(X_test).any():\n",
        "                     X_test = imp.transform(X_test)\n",
        "\n",
        "        # Run Parallel Iterations\n",
        "        metrics = Parallel(n_jobs=N_JOBS)(delayed(run_evaluation)\n",
        "                                          (i, X_train, y_train, X_test, y_test, is_same)\n",
        "                                          for i in range(NUM_ITERATIONS))\n",
        "\n",
        "        metrics = np.array(metrics)\n",
        "        means = np.mean(metrics, axis=0)\n",
        "\n",
        "        # Add to Table\n",
        "        n_dim = X_train.shape[1]\n",
        "        results_table.append({\n",
        "            \"Configuration\": f\"{config_label} ({n_dim})\",\n",
        "            \"SRCC\": means[0],\n",
        "            \"KRCC\": means[1],\n",
        "            \"PLCC\": means[2],\n",
        "            \"RMSE\": means[3]\n",
        "        })\n",
        "\n",
        "    # 6. Display Results\n",
        "    if results_table:\n",
        "        res_df = pd.DataFrame(results_table)\n",
        "        res_df = res_df.sort_values(by=\"SRCC\", ascending=False)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(f\"RESULTS TABLE ({NUM_ITERATIONS} Iterations)\")\n",
        "        print(f\"Train: {t_name} | Test: {test_name}\")\n",
        "        print(\"=\"*70)\n",
        "        print(res_df.to_string(index=False, float_format=\"%.4f\"))\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        csv_name = f\"Results_{t_name}_vs_{test_name}.csv\"\n",
        "        res_df.to_csv(csv_name, index=False)\n",
        "        print(f\"Saved to {csv_name}\")\n",
        "    else:\n",
        "        print(\"No valid results computed.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install xgboost"
      ],
      "metadata": {
        "id": "07RucrkfldCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io\n",
        "import scipy.stats\n",
        "from scipy.optimize import curve_fit\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.impute import SimpleImputer\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm import tqdm\n",
        "import xgboost as xgb\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURATION\n",
        "# ==========================================\n",
        "NUM_ITERATIONS = 500\n",
        "N_JOBS = -1\n",
        "\n",
        "# XGBoost Params\n",
        "# REMOVED 'random_state' to avoid duplication\n",
        "# REMOVED 'early_stopping_rounds' from here (we add it in the loop)\n",
        "XGB_PARAMS = {\n",
        "    'n_estimators': 500,\n",
        "    'max_depth': 4,\n",
        "    'learning_rate': 0.05,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'objective': 'reg:squarederror',\n",
        "    'n_jobs': 1, # Keep this 1 to allow outer parallelization\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# HELPER FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "def get_dataset_info(prompt_text):\n",
        "    print(f\"\\n--- {prompt_text} ---\")\n",
        "    print(\"1. UID (Expects 'mos_UID.xlsx')\")\n",
        "    print(\"2. SAUD (Expects 'SAUD_MOS.xlsx')\")\n",
        "    choice = input(\"Enter 1 or 2: \").strip()\n",
        "    if choice == '1':\n",
        "        return \"UID\", \"mos_UID.xlsx\", input(\"Enter UID ZIP: \").strip()\n",
        "    elif choice == '2':\n",
        "        return \"SAUD\", \"SAUD_MOS.xlsx\", input(\"Enter SAUD ZIP: \").strip()\n",
        "    return None, None, None\n",
        "\n",
        "def unzip_features(zip_name, extract_to):\n",
        "    if os.path.exists(extract_to):\n",
        "        import shutil\n",
        "        shutil.rmtree(extract_to)\n",
        "    os.makedirs(extract_to)\n",
        "    with zipfile.ZipFile(zip_name, 'r') as z:\n",
        "        z.extractall(extract_to)\n",
        "\n",
        "def load_features_from_folder(feature_folder, df_mos, name_col, mos_col):\n",
        "    file_map = {}\n",
        "    for root, _, files in os.walk(feature_folder):\n",
        "        for f in files:\n",
        "            if f.endswith('.mat'):\n",
        "                file_map[f.lower()] = os.path.join(root, f)\n",
        "                file_map[os.path.splitext(f)[0].lower()] = os.path.join(root, f)\n",
        "\n",
        "    features, scores = [], []\n",
        "    for _, row in df_mos.iterrows():\n",
        "        fname = str(row[name_col]).strip()\n",
        "        key = os.path.basename(fname).lower()\n",
        "        path = file_map.get(key) or file_map.get(os.path.splitext(key)[0])\n",
        "        if path:\n",
        "            try:\n",
        "                mat = scipy.io.loadmat(path)\n",
        "                k = [k for k in mat.keys() if not k.startswith('__')][0]\n",
        "                features.append(np.array(mat[k]).flatten())\n",
        "                scores.append(row[mos_col])\n",
        "            except: pass\n",
        "    return np.array(features), np.array(scores)\n",
        "\n",
        "def logistic_func(X, b1, b2, b3, b4):\n",
        "    logisticPart = 1 + np.exp(-(X - b3) / np.abs(b4))\n",
        "    return b2 + (b1 - b2) / logisticPart\n",
        "\n",
        "def compute_metrics(y_pred, y):\n",
        "    try:\n",
        "        beta_init = [np.max(y), np.min(y), np.mean(y_pred), 0.5]\n",
        "        popt, _ = curve_fit(logistic_func, y_pred, y, p0=beta_init, maxfev=int(1e4))\n",
        "        y_pred_logistic = logistic_func(y_pred, *popt)\n",
        "    except:\n",
        "        y_pred_logistic = y_pred\n",
        "\n",
        "    SRCC = scipy.stats.spearmanr(y, y_pred)[0]\n",
        "    try: KRCC = scipy.stats.kendalltau(y, y_pred)[0]\n",
        "    except: KRCC = 0\n",
        "    PLCC = scipy.stats.pearsonr(y, y_pred_logistic)[0]\n",
        "    RMSE = np.sqrt(mean_squared_error(y, y_pred_logistic))\n",
        "    return [SRCC, KRCC, PLCC, RMSE]\n",
        "\n",
        "# ==========================================\n",
        "# CORE EVALUATION (FIXED FOR XGBOOST 2.0+)\n",
        "# ==========================================\n",
        "\n",
        "def run_evaluation_xgboost(iter_idx, X_train_full, y_train_full, X_test_full, y_test_full, is_same_dataset):\n",
        "\n",
        "    np.random.seed(iter_idx)\n",
        "\n",
        "    # --- SPLIT LOGIC ---\n",
        "    if is_same_dataset:\n",
        "        # INTRA: 70% Train, 10% Val, 20% Test\n",
        "        n = len(y_train_full)\n",
        "        perm = np.random.permutation(n)\n",
        "\n",
        "        n_train = int(n * 0.7)\n",
        "        n_val   = int(n * 0.1)\n",
        "\n",
        "        train_idx = perm[:n_train]\n",
        "        val_idx   = perm[n_train : n_train + n_val]\n",
        "        test_idx  = perm[n_train + n_val :]\n",
        "\n",
        "        X_tr, y_tr = X_train_full[train_idx], y_train_full[train_idx]\n",
        "        X_val, y_val = X_train_full[val_idx], y_train_full[val_idx]\n",
        "        X_te, y_te = X_train_full[test_idx], y_train_full[test_idx]\n",
        "\n",
        "    else:\n",
        "        # INTER: 90% Train, 10% Val -> Test on B\n",
        "        n = len(y_train_full)\n",
        "        perm = np.random.permutation(n)\n",
        "\n",
        "        n_train = int(n * 0.9)\n",
        "\n",
        "        train_idx = perm[:n_train]\n",
        "        val_idx   = perm[n_train:]\n",
        "\n",
        "        X_tr, y_tr = X_train_full[train_idx], y_train_full[train_idx]\n",
        "        X_val, y_val = X_train_full[val_idx], y_train_full[val_idx]\n",
        "        X_te, y_te = X_test_full, y_test_full\n",
        "\n",
        "    # --- MODEL TRAINING ---\n",
        "    # FIX: Move 'early_stopping_rounds' to constructor\n",
        "    model = xgb.XGBRegressor(\n",
        "        **XGB_PARAMS,\n",
        "        early_stopping_rounds=20,\n",
        "        random_state=iter_idx\n",
        "    )\n",
        "\n",
        "    # FIX: Remove 'early_stopping_rounds' from fit()\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    preds = model.predict(X_te)\n",
        "    return compute_metrics(preds, y_te)\n",
        "\n",
        "# ==========================================\n",
        "# MAIN\n",
        "# ==========================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=== XGBoost Analysis (70/10/20 Intra & 90/10 Inter) ===\")\n",
        "\n",
        "    # 1. Inputs\n",
        "    t_name, t_mos_file, t_zip = get_dataset_info(\"SELECT TRAINING DATASET\")\n",
        "    if not t_name: exit()\n",
        "\n",
        "    test_name, test_mos_file, test_zip = get_dataset_info(\"SELECT TESTING DATASET\")\n",
        "    if not test_name: exit()\n",
        "\n",
        "    is_same = (t_name == test_name) and (t_zip == test_zip)\n",
        "\n",
        "    if is_same:\n",
        "        print(f\"\\n[Mode] Same Dataset -> Splitting {t_name}: 70% Train, 10% Val, 20% Test\")\n",
        "    else:\n",
        "        print(f\"\\n[Mode] Cross Dataset -> {t_name}: 90% Train, 10% Val | Test on {test_name}\")\n",
        "\n",
        "    # 2. Unzip\n",
        "    train_extract_path = \"./train_feats_extracted\"\n",
        "    test_extract_path = \"./test_feats_extracted\"\n",
        "\n",
        "    unzip_features(t_zip, train_extract_path)\n",
        "    if not is_same:\n",
        "        unzip_features(test_zip, test_extract_path)\n",
        "    else:\n",
        "        test_extract_path = train_extract_path\n",
        "\n",
        "    # Load MOS\n",
        "    df_train = pd.read_excel(t_mos_file)\n",
        "    df_train.columns = [c.strip() for c in df_train.columns]\n",
        "    df_test = pd.read_excel(test_mos_file)\n",
        "    df_test.columns = [c.strip() for c in df_test.columns]\n",
        "\n",
        "    def get_cols(df):\n",
        "        if 'Image' in df.columns: name = 'Image'\n",
        "        elif 'image_name' in df.columns: name = 'image_name'\n",
        "        else: name = df.columns[0]\n",
        "        return name, 'MOS'\n",
        "\n",
        "    t_name_col, t_mos_col = get_cols(df_train)\n",
        "    test_name_col, test_mos_col = get_cols(df_test)\n",
        "\n",
        "    # 3. Detect Folder Structure\n",
        "    root_items = [f.path for f in os.scandir(train_extract_path) if f.is_dir()]\n",
        "    if len(root_items) == 1 and os.path.basename(root_items[0]) not in ['H','S','V','HS']:\n",
        "         train_extract_path = root_items[0]\n",
        "         if not is_same:\n",
        "             test_items = [f.path for f in os.scandir(test_extract_path) if f.is_dir()]\n",
        "             if len(test_items) == 1: test_extract_path = test_items[0]\n",
        "\n",
        "    items = [f.name for f in os.scandir(train_extract_path) if f.is_dir()]\n",
        "    potential_modes = ['H', 'S', 'V', 'HS']\n",
        "    has_modes = any(m in items for m in potential_modes)\n",
        "    experiments = []\n",
        "\n",
        "    if has_modes:\n",
        "        for mode in items:\n",
        "            if mode not in potential_modes: continue\n",
        "            mode_path = os.path.join(train_extract_path, mode)\n",
        "            for cfg in [f.name for f in os.scandir(mode_path) if f.is_dir()]:\n",
        "                experiments.append({\n",
        "                    'label': f\"{mode} - {cfg}\",\n",
        "                    'train_path': os.path.join(train_extract_path, mode, cfg),\n",
        "                    'test_path': os.path.join(test_extract_path, mode, cfg) if test_extract_path else None\n",
        "                })\n",
        "    else:\n",
        "        for cfg in items:\n",
        "            experiments.append({\n",
        "                'label': cfg,\n",
        "                'train_path': os.path.join(train_extract_path, cfg),\n",
        "                'test_path': os.path.join(test_extract_path, cfg) if test_extract_path else None\n",
        "            })\n",
        "\n",
        "    experiments.sort(key=lambda x: x['label'])\n",
        "\n",
        "    # 4. Run Loop\n",
        "    results_table = []\n",
        "    print(f\"\\nEvaluating {len(experiments)} feature sets...\")\n",
        "\n",
        "    for exp in tqdm(experiments):\n",
        "        t_folder = exp['train_path']\n",
        "        test_folder = exp['test_path']\n",
        "\n",
        "        X_train, y_train = load_features_from_folder(t_folder, df_train, t_name_col, t_mos_col)\n",
        "\n",
        "        if is_same:\n",
        "            X_test, y_test = X_train, y_train\n",
        "        else:\n",
        "            if not test_folder or not os.path.exists(test_folder): continue\n",
        "            X_test, y_test = load_features_from_folder(test_folder, df_test, test_name_col, test_mos_col)\n",
        "\n",
        "        if len(X_train) == 0 or len(X_test) == 0: continue\n",
        "\n",
        "        # Impute\n",
        "        if np.isnan(X_train).any():\n",
        "            imp = SimpleImputer(strategy='mean')\n",
        "            X_train = imp.fit_transform(X_train)\n",
        "            X_test = X_train if is_same else imp.transform(X_test)\n",
        "\n",
        "        metrics = Parallel(n_jobs=N_JOBS)(\n",
        "            delayed(run_evaluation_xgboost)(\n",
        "                i, X_train, y_train, X_test, y_test, is_same\n",
        "            )\n",
        "            for i in range(NUM_ITERATIONS)\n",
        "        )\n",
        "\n",
        "        means = np.mean(metrics, axis=0)\n",
        "        results_table.append({\n",
        "            \"Configuration\": exp['label'],\n",
        "            \"SRCC\": means[0],\n",
        "            \"KRCC\": means[1],\n",
        "            \"PLCC\": means[2],\n",
        "            \"RMSE\": means[3]\n",
        "        })\n",
        "\n",
        "    # 5. Save\n",
        "    if results_table:\n",
        "        res_df = pd.DataFrame(results_table).sort_values(by=\"SRCC\", ascending=False)\n",
        "        print(\"\\n\" + res_df.to_string(index=False, float_format=\"%.4f\"))\n",
        "        res_df.to_csv(f\"Results_XGBoost_{t_name}_{test_name}.csv\", index=False)\n",
        "        print(\"Done.\")"
      ],
      "metadata": {
        "id": "bBx1AXGNle7q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}