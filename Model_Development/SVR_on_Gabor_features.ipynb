{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlaR-_P7V9EZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import tempfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "from tqdm import tqdm\n",
        "from itertools import combinations\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "from scipy.io import loadmat\n",
        "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
        "from scipy.optimize import curve_fit, OptimizeWarning\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "UID_FEATURES_ZIP  = r'UID_features_Gabor.zip'\n",
        "SAUD_FEATURES_ZIP = r'SAUD_features_Gabor.zip'\n",
        "\n",
        "UID_MOS_FILE  = r'mos_UID.xlsx'\n",
        "SAUD_MOS_FILE = r'SAUD_MOS.xlsx'\n",
        "\n",
        "NUM_FEATURES = 12\n",
        "NUM_ITER = 500\n",
        "\n",
        "TRAIN_RATIO_INTRA = 0.7\n",
        "\n",
        "OUTPUT_EXCEL = 'Gabor_66Combo_SVR_Ranked_Optimized.xlsx'\n",
        "\n",
        "ORIENTATIONS = [0, 30, 60, 90, 120, 150]\n",
        "\n",
        "def logistic_5p(x, b1, b2, b3, b4, b5):\n",
        "    logistic_part = 0.5 - 1 / (1 + np.exp(np.clip(b2 * (x - b3), -100, 100)))\n",
        "    return b1 * logistic_part + b4 * x + b5\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    # Fast check for constant prediction to avoid errors\n",
        "    if np.std(y_pred) < 1e-6:\n",
        "        return (\n",
        "            pearsonr(y_true, y_pred)[0],\n",
        "            spearmanr(y_true, y_pred)[0],\n",
        "            kendalltau(y_true, y_pred)[0],\n",
        "            np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "        )\n",
        "\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\", OptimizeWarning)\n",
        "        try:\n",
        "            p0 = [np.ptp(y_true), 1, np.mean(y_pred), 0, np.mean(y_true)]\n",
        "            popt, _ = curve_fit(logistic_5p, y_pred, y_true, p0=p0, maxfev=1000) # Reduced maxfev slightly for speed\n",
        "            y_pred = logistic_5p(y_pred, *popt)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return (\n",
        "        pearsonr(y_true, y_pred)[0],\n",
        "        spearmanr(y_true, y_pred)[0],\n",
        "        kendalltau(y_true, y_pred)[0],\n",
        "        np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    )\n",
        "\n",
        "def load_features_from_zip(zip_path, mos_file):\n",
        "    mos_df = pd.read_excel(mos_file) if mos_file.endswith(('xls','xlsx')) else pd.read_csv(mos_file)\n",
        "    cols = {c.lower(): c for c in mos_df.columns}\n",
        "\n",
        "    name_col = cols.get('name', cols.get('image_name'))\n",
        "    mos_col = cols.get('mos')\n",
        "\n",
        "    def clean_name(x):\n",
        "        return os.path.splitext(os.path.basename(str(x).strip()))[0]\n",
        "\n",
        "    mos_df['key'] = mos_df[name_col].apply(clean_name)\n",
        "    mos_dict = dict(zip(mos_df['key'], mos_df[mos_col]))\n",
        "\n",
        "    X, y = [], []\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "            z.extractall(tmpdir)\n",
        "\n",
        "        for root, _, files in os.walk(tmpdir):\n",
        "            for f in files:\n",
        "                if f.endswith('.mat'):\n",
        "                    key = os.path.splitext(f)[0]\n",
        "                    if key in mos_dict:\n",
        "                        FG = loadmat(os.path.join(root, f))['FGabor'].reshape(-1)\n",
        "                        if FG.size == NUM_FEATURES:\n",
        "                            X.append(FG)\n",
        "                            y.append(mos_dict[key])\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def process_combination(combo_key, X_subset, y_data, train_ratio, n_iter):\n",
        "    \"\"\"\n",
        "    Runs the SVR iterations for a single feature combination.\n",
        "    This function is executed in parallel workers.\n",
        "    \"\"\"\n",
        "    plcc_l, srcc_l, krcc_l, rmse_l = [], [], [], []\n",
        "\n",
        "    svr_params = {'kernel': 'rbf', 'C': 100, 'gamma': 'scale', 'epsilon': 0.01}\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "            X_subset, y_data, train_size=train_ratio, random_state=i\n",
        "        )\n",
        "\n",
        "        model = SVR(**svr_params)\n",
        "        model.fit(X_tr, y_tr)\n",
        "\n",
        "        y_pred = model.predict(X_te)\n",
        "\n",
        "        plcc, srcc, krcc, rmse = compute_metrics(y_te, y_pred)\n",
        "\n",
        "        plcc_l.append(plcc)\n",
        "        srcc_l.append(srcc)\n",
        "        krcc_l.append(krcc)\n",
        "        rmse_l.append(rmse)\n",
        "\n",
        "    f1, f2 = map(lambda x: int(x[1:]), combo_key.split('_'))\n",
        "\n",
        "    return {\n",
        "        'Combination': combo_key,\n",
        "        'F1_Scale': 1 if f1 <= 6 else 2,\n",
        "        'F1_Orientation': ORIENTATIONS[(f1-1) % 6],\n",
        "        'F2_Scale': 1 if f2 <= 6 else 2,\n",
        "        'F2_Orientation': ORIENTATIONS[(f2-1) % 6],\n",
        "        'PLCC': np.mean(plcc_l),\n",
        "        'SRCC': np.mean(srcc_l),\n",
        "        'KRCC': np.mean(krcc_l),\n",
        "        'RMSE': np.mean(rmse_l)\n",
        "    }\n",
        "\n",
        "def evaluate_combinations_optimized(X_data, y_data, train_ratio, task_name):\n",
        "    combos = {\n",
        "        f'f{i+1}_f{j+1}': X_data[:, [i, j]]\n",
        "        for i, j in combinations(range(NUM_FEATURES), 2)\n",
        "    }\n",
        "\n",
        "    print(f\"Starting parallel evaluation for: {task_name} ({len(combos)} combinations)...\")\n",
        "\n",
        "    results = Parallel(n_jobs=-1)(\n",
        "        delayed(process_combination)(\n",
        "            combo_key,\n",
        "            X_subset,\n",
        "            y_data,\n",
        "            train_ratio,\n",
        "            NUM_ITER\n",
        "        )\n",
        "        for combo_key, X_subset in tqdm(combos.items(), desc=task_name)\n",
        "    )\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    return df.sort_values(by='PLCC', ascending=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Loading datasets...\")\n",
        "    X_uid, y_uid = load_features_from_zip(UID_FEATURES_ZIP, UID_MOS_FILE)\n",
        "    X_saud, y_saud = load_features_from_zip(SAUD_FEATURES_ZIP, SAUD_MOS_FILE)\n",
        "\n",
        "    print(\"\\nRunning evaluations (Optimized)...\")\n",
        "\n",
        "    # 1. UID -> UID\n",
        "    df_uid_uid = evaluate_combinations_optimized(\n",
        "        X_uid, y_uid,\n",
        "        TRAIN_RATIO_INTRA, \"UID (Intra)\"\n",
        "    )\n",
        "\n",
        "    # 2. SAUD -> SAUD\n",
        "    df_saud_saud = evaluate_combinations_optimized(\n",
        "        X_saud, y_saud,\n",
        "        TRAIN_RATIO_INTRA, \"SAUD (Intra)\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with pd.ExcelWriter('Gabor_66Combo_Recovered.xlsx') as writer:\n",
        "    df_uid_uid.to_excel(writer, sheet_name='UID_UID', index=False)\n",
        "    df_saud_saud.to_excel(writer, sheet_name='SAUD_SAUD', index=False)\n",
        "\n",
        "print(\" Data recovered, saved to Gabor_66Combo_Recovered.xlsx\")"
      ],
      "metadata": {
        "id": "Rs0IPYkVUdNL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}