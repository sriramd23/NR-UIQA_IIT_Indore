{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKdhVcQCIebO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "zip_filename = 'features.zip'\n",
        "extract_path = '/content/features_data'\n",
        "\n",
        "# Check and Unzip\n",
        "if os.path.exists(zip_filename):\n",
        "    if not os.path.exists(extract_path):\n",
        "        print(f\"Unzipping {zip_filename}...\")\n",
        "        with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_path)\n",
        "    else:\n",
        "        print(\"Files already extracted.\")\n",
        "else:\n",
        "    print(f\"ERROR: '{zip_filename}' not found! Please upload it to the Files sidebar.\")\n",
        "\n",
        "uid_excel = 'mos_UID.xlsx'\n",
        "saud_excel = 'SAUD_MOS.xlsx'\n",
        "feature_root = os.path.abspath(extract_path)\n",
        "\n",
        "X, y = [], []\n",
        "\n",
        "def clean_name(fname):\n",
        "    return fname.strip(\"'\").strip() if isinstance(fname, str) else fname\n",
        "\n",
        "# --- Load UID ---\n",
        "if os.path.exists(uid_excel):\n",
        "    try:\n",
        "        df = pd.read_excel(uid_excel)\n",
        "        uid_map = dict(zip(df['Name'], df['MOS']))\n",
        "        all_files = glob.glob(os.path.join(feature_root, '**', '*.mat'), recursive=True)\n",
        "        count = 0\n",
        "        for fpath in all_files:\n",
        "            img_name = os.path.splitext(os.path.basename(fpath))[0] + \".png\"\n",
        "            if img_name in uid_map:\n",
        "                try:\n",
        "                    mat = scipy.io.loadmat(fpath)\n",
        "                    if 'features' in mat:\n",
        "                        feat = mat['features'].flatten()\n",
        "                        if feat.shape[0] == 25:\n",
        "                            X.append(feat)\n",
        "                            y.append(uid_map[img_name])\n",
        "                            count += 1\n",
        "                except: pass\n",
        "        print(f\"UID Data: Loaded {count} samples.\")\n",
        "    except: print(\"Error reading UID Excel.\")\n",
        "\n",
        "# --- Load SAUD ---\n",
        "if os.path.exists(saud_excel):\n",
        "    try:\n",
        "        df = pd.read_excel(saud_excel)\n",
        "        df['clean'] = df['Name'].apply(clean_name)\n",
        "        saud_map = dict(zip(df['clean'], df['MOS']))\n",
        "        all_files = glob.glob(os.path.join(feature_root, '**', '*.mat'), recursive=True)\n",
        "        count = 0\n",
        "        for fpath in all_files:\n",
        "            parts = fpath.replace('\\\\', '/').split('/')\n",
        "            if len(parts) > 3:\n",
        "                name = os.path.splitext(parts[-1])[0] + \".png\"\n",
        "                key = f\"{parts[-3]}/{parts[-2]}/{name}\"\n",
        "                score = saud_map.get(key) or saud_map.get(name)\n",
        "                if score is not None:\n",
        "                    try:\n",
        "                        mat = scipy.io.loadmat(fpath)\n",
        "                        if 'features' in mat:\n",
        "                            feat = mat['features'].flatten()\n",
        "                            if feat.shape[0] == 25:\n",
        "                                X.append(feat)\n",
        "                                y.append(score)\n",
        "                                count += 1\n",
        "                    except: pass\n",
        "        print(f\"SAUD Data: Loaded {count} samples.\")\n",
        "    except: print(\"Error reading SAUD Excel.\")\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "print(f\"\\nTOTAL DATA: {len(X)} samples loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**simulation engine**"
      ],
      "metadata": {
        "id": "pPS9zOMYM8_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "ITERATIONS = 500\n",
        "\n",
        "def run_simulation(model_class, param_grid, X, y, n_iters=10, scale=False, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Runs repeated random sub-sampling validation.\n",
        "    \"\"\"\n",
        "    metrics = {'PLCC': [], 'SRCC': [], 'KRCC': [], 'RMSE': []}\n",
        "    print(f\"Starting {n_iters} iterations for {model_name}...\")\n",
        "\n",
        "    for i in tqdm(range(n_iters)):\n",
        "        # 1. Random Split (80% Train+Val, 20% Test)\n",
        "        X_tv, X_test, y_tv, y_test = train_test_split(X, y, test_size=0.20, random_state=None)\n",
        "\n",
        "        # 2. Validation Split (12.5% of 80% = 10% of total)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_tv, y_tv, test_size=0.125, random_state=None)\n",
        "\n",
        "        # 3. Scaling (Only if scale=True, mostly for SVR)\n",
        "        if scale:\n",
        "            scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "            X_train_p = scaler.fit_transform(X_train)\n",
        "            X_val_p = scaler.transform(X_val)\n",
        "            X_test_p = scaler.transform(X_test)\n",
        "        else:\n",
        "            X_train_p, X_val_p, X_test_p = X_train, X_val, X_test\n",
        "\n",
        "        # 4. Grid Search\n",
        "        best_score = -2\n",
        "        best_model = None\n",
        "\n",
        "        if not param_grid:\n",
        "            model = model_class()\n",
        "            model.fit(X_train_p, y_train)\n",
        "            best_model = model\n",
        "        else:\n",
        "            for params in ParameterGrid(param_grid):\n",
        "                model = model_class(**params)\n",
        "                model.fit(X_train_p, y_train)\n",
        "                # Validation Metric: SRCC\n",
        "                srcc, _ = spearmanr(y_val, model.predict(X_val_p))\n",
        "                if srcc > best_score:\n",
        "                    best_score = srcc\n",
        "                    best_model = model\n",
        "\n",
        "        # 5. Final Test\n",
        "        test_preds = best_model.predict(X_test_p)\n",
        "        metrics['PLCC'].append(pearsonr(y_test, test_preds)[0])\n",
        "        metrics['SRCC'].append(spearmanr(y_test, test_preds)[0])\n",
        "        metrics['KRCC'].append(kendalltau(y_test, test_preds)[0])\n",
        "        metrics['RMSE'].append(np.sqrt(mean_squared_error(y_test, test_preds)))\n",
        "\n",
        "    return {k: np.mean(v) for k, v in metrics.items()}"
      ],
      "metadata": {
        "id": "ZeGfYv69Lo6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Support Vector Regression Method**"
      ],
      "metadata": {
        "id": "IXY1DVuNL3kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "# Hyperparameters to tune\n",
        "param_svr = {\n",
        "    'C': [10, 50, 100],\n",
        "    'gamma': ['scale', 0.1],\n",
        "    'epsilon': [0.1]\n",
        "}\n",
        "\n",
        "# Run Simulation\n",
        "svr_avg = run_simulation(\n",
        "    SVR,\n",
        "    param_svr,\n",
        "    X, y,\n",
        "    n_iters=ITERATIONS,\n",
        "    scale=True,\n",
        "    model_name=\"SVR\"\n",
        ")\n",
        "\n",
        "print(f\"SVR Result: {svr_avg}\")"
      ],
      "metadata": {
        "id": "koZMj1pSLzr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest Regression Method**"
      ],
      "metadata": {
        "id": "9RzsYMPSMDSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Hyperparameters\n",
        "param_rf = {\n",
        "    'n_estimators': [100],\n",
        "    'max_depth': [None, 10],\n",
        "    'n_jobs': [-1]\n",
        "}\n",
        "\n",
        "# Run Simulation\n",
        "rf_avg = run_simulation(\n",
        "    RandomForestRegressor,\n",
        "    param_rf,\n",
        "    X, y,\n",
        "    n_iters=ITERATIONS,\n",
        "    scale=False,\n",
        "    model_name=\"Random Forest\"\n",
        ")\n",
        "\n",
        "print(f\"RF Result: {rf_avg}\")"
      ],
      "metadata": {
        "id": "vCUDkjgKL-oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AdaBoost Regressor**"
      ],
      "metadata": {
        "id": "N8wG55CuCxwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "\n",
        "# Hyperparameters\n",
        "param_ada = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'learning_rate': [0.01, 0.1, 1.0],\n",
        "    'loss': ['linear', 'square'],\n",
        "    'random_state': [None]\n",
        "}\n",
        "\n",
        "# Run Simulation\n",
        "ada_avg = run_simulation(\n",
        "    AdaBoostRegressor,\n",
        "    param_ada,\n",
        "    X, y,\n",
        "    n_iters=ITERATIONS,\n",
        "    scale=False,\n",
        "    model_name=\"AdaBoost\"\n",
        ")\n",
        "\n",
        "print(f\"AdaBoost Result: {ada_avg}\")"
      ],
      "metadata": {
        "id": "WwV_77llCt9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Boosting Regressor**"
      ],
      "metadata": {
        "id": "ZZhUSJyGC8dM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Hyperparameters\n",
        "param_gbm = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'learning_rate': [0.05, 0.1],\n",
        "    'max_depth': [3, 5],\n",
        "    'subsample': [0.8]\n",
        "}\n",
        "\n",
        "# Run Simulation\n",
        "gbm_avg = run_simulation(\n",
        "    GradientBoostingRegressor,\n",
        "    param_gbm,\n",
        "    X, y,\n",
        "    n_iters=ITERATIONS,\n",
        "    scale=False,\n",
        "    model_name=\"Gradient Boosting\"\n",
        ")\n",
        "\n",
        "print(f\"GBM Result: {gbm_avg}\")"
      ],
      "metadata": {
        "id": "7NDPMbBhC7-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**XGBoost Regressor**"
      ],
      "metadata": {
        "id": "ozfxQz2NMh0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Hyperparameters\n",
        "param_xgb = {\n",
        "    'n_estimators': [100, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 5],\n",
        "    'subsample': [0.8],\n",
        "    'colsample_bytree': [0.8],\n",
        "    'objective': ['reg:squarederror'],\n",
        "    'n_jobs': [-1]\n",
        "}\n",
        "\n",
        "# Run Simulation\n",
        "xgb_avg = run_simulation(\n",
        "    xgb.XGBRegressor,\n",
        "    param_xgb,\n",
        "    X, y,\n",
        "    n_iters=ITERATIONS,\n",
        "    scale=False,\n",
        "    model_name=\"XGBoost\"\n",
        ")\n",
        "\n",
        "print(f\"XGBoost Result: {xgb_avg}\")"
      ],
      "metadata": {
        "id": "VllUNoseMtSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Comparison**"
      ],
      "metadata": {
        "id": "0DSxfMJxMwHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame\n",
        "results_df = pd.DataFrame(\n",
        "    [svr_avg, rf_avg, ada_avg, gbm_avg, xgb_avg],\n",
        "    index=['SVR', 'Random Forest', 'AdaBoost', 'Gradient Boost', 'XGBoost']\n",
        ")\n",
        "results_df = results_df[['PLCC', 'SRCC', 'KRCC', 'RMSE']] # Reorder columns\n",
        "\n",
        "# 1. Print Text Table\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"FINAL PERFORMANCE (Average of {ITERATIONS} Iterations)\")\n",
        "print(\"=\"*50)\n",
        "print(results_df.round(4))\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 2. Generate Graphic Table\n",
        "fig, ax = plt.subplots(figsize=(10, 4))\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "\n",
        "table_data = results_df.round(4)\n",
        "table = ax.table(\n",
        "    cellText=table_data.values,\n",
        "    colLabels=table_data.columns,\n",
        "    rowLabels=table_data.index,\n",
        "    cellLoc='center',\n",
        "    loc='center'\n",
        ")\n",
        "\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(12)\n",
        "table.scale(1.2, 1.8) # Adjust size\n",
        "\n",
        "plt.title(f\"Model Comparison (N={ITERATIONS})\", fontsize=16, weight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8opJ3VvEM5qI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}