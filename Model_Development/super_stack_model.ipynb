{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70acOykee-Hs"
      },
      "outputs": [],
      "source": [
        "# Install missing libraries\n",
        "!pip install lightgbm catboost\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from tqdm.notebook import tqdm # Improved progress bar for Jupyter/Colab\n",
        "\n",
        "# Scikit-Learn\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import RidgeCV, ElasticNetCV, LinearRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import (RandomForestRegressor, ExtraTreesRegressor,\n",
        "                              GradientBoostingRegressor, StackingRegressor)\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "# Boosting Libraries\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "# Metrics\n",
        "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
        "\n",
        "# Settings\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('ggplot')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- 1. ZIP EXTRACTION ----------------\n",
        "zip_filename = 'features.zip'\n",
        "extract_path = '/content/features_data'\n",
        "\n",
        "if os.path.exists(zip_filename):\n",
        "    if not os.path.exists(extract_path):\n",
        "        print(f\"Unzipping {zip_filename}...\")\n",
        "        with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_path)\n",
        "    else:\n",
        "        print(\"Files already extracted.\")\n",
        "else:\n",
        "    print(f\"WARNING: {zip_filename} not found. Please upload it to Colab.\")\n",
        "\n",
        "feature_root = os.path.abspath(extract_path)\n",
        "\n",
        "# ---------------- 2. LOAD MOS FILES ----------------\n",
        "uid_excel  = 'mos_UID.xlsx'\n",
        "saud_excel = 'SAUD_MOS.xlsx'\n",
        "\n",
        "X_list, y_list, dataset_id_list = [], [], []\n",
        "\n",
        "# ---------------- 3. PROCESS UID DATASET ----------------\n",
        "if os.path.exists(uid_excel):\n",
        "    df_uid = pd.read_excel(uid_excel)\n",
        "    uid_map = dict(zip(df_uid['Name'], df_uid['MOS']))\n",
        "\n",
        "    all_files = glob.glob(os.path.join(feature_root, '**', '*.mat'), recursive=True)\n",
        "    count_uid = 0\n",
        "\n",
        "    for fpath in all_files:\n",
        "        img_name = os.path.splitext(os.path.basename(fpath))[0] + \".png\"\n",
        "        if img_name in uid_map:\n",
        "            try:\n",
        "                mat = scipy.io.loadmat(fpath)\n",
        "                if 'features' in mat:\n",
        "                    feat = mat['features'].flatten()\n",
        "                    if feat.shape[0] == 25:\n",
        "                        X_list.append(feat)\n",
        "                        y_list.append(uid_map[img_name])\n",
        "                        dataset_id_list.append(0)  # ID 0 for UID\n",
        "                        count_uid += 1\n",
        "            except:\n",
        "                pass\n",
        "    print(f\"UID Data Loaded: {count_uid} samples\")\n",
        "\n",
        "# ---------------- 4. PROCESS SAUD DATASET ----------------\n",
        "if os.path.exists(saud_excel):\n",
        "    df_saud = pd.read_excel(saud_excel)\n",
        "    def clean_name(fname):\n",
        "        return fname.strip(\"'\").strip() if isinstance(fname, str) else fname\n",
        "\n",
        "    df_saud['clean'] = df_saud['Name'].apply(clean_name)\n",
        "    saud_map = dict(zip(df_saud['clean'], df_saud['MOS']))\n",
        "\n",
        "    count_saud = 0\n",
        "    # Reuse file list for efficiency\n",
        "    for fpath in all_files:\n",
        "        parts = fpath.replace('\\\\', '/').split('/')\n",
        "        if len(parts) > 3:\n",
        "            name = os.path.splitext(parts[-1])[0] + \".png\"\n",
        "            key = f\"{parts[-3]}/{parts[-2]}/{name}\"\n",
        "\n",
        "            score = saud_map.get(key) or saud_map.get(name)\n",
        "            if score is not None:\n",
        "                try:\n",
        "                    mat = scipy.io.loadmat(fpath)\n",
        "                    if 'features' in mat:\n",
        "                        feat = mat['features'].flatten()\n",
        "                        if feat.shape[0] == 25:\n",
        "                            X_list.append(feat)\n",
        "                            y_list.append(score)\n",
        "                            dataset_id_list.append(1)  # ID 1 for SAUD\n",
        "                            count_saud += 1\n",
        "                except:\n",
        "                    pass\n",
        "    print(f\"SAUD Data Loaded: {count_saud} samples\")\n",
        "\n",
        "# ---------------- 5. FINALIZE ARRAYS ----------------\n",
        "if len(X_list) > 0:\n",
        "    X = np.array(X_list)\n",
        "    y = np.array(y_list)\n",
        "    ids = np.array(dataset_id_list).reshape(-1, 1)\n",
        "\n",
        "    # Append Dataset ID as the 26th Feature\n",
        "    X = np.hstack([X, ids])\n",
        "\n",
        "    # Handle NaNs/Infs\n",
        "    X = np.where(np.isinf(X), np.nan, X)\n",
        "    if np.isnan(X).any():\n",
        "        col_mean = np.nanmean(X, axis=0)\n",
        "        inds = np.where(np.isnan(X))\n",
        "        X[inds] = np.take(col_mean, inds[1])\n",
        "\n",
        "    print(f\"\\nRAW DATA SHAPE: {X.shape}\")\n",
        "else:\n",
        "    print(\"ERROR: No data loaded.\")"
      ],
      "metadata": {
        "id": "wkhmV0cNfClb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def enhance_features(X_in):\n",
        "    # 1. Log Transform: Handles heavy-tailed image stats\n",
        "    # Use log1p(abs(x)) to be safe against zeros and negatives\n",
        "    X_log = np.log1p(np.abs(X_in))\n",
        "\n",
        "    # 2. Square Transform: Captures quadratic relationships\n",
        "    X_sq = X_in ** 2\n",
        "\n",
        "    # Combine: [Original, Log, Square]\n",
        "    X_out = np.hstack([X_in, X_log, X_sq])\n",
        "\n",
        "    # Sanity check for numbers\n",
        "    X_out = np.where(np.isinf(X_out), 0, X_out)\n",
        "    X_out = np.where(np.isnan(X_out), 0, X_out)\n",
        "\n",
        "    return X_out\n",
        "\n",
        "print(\"Enhancing features with Log and Square transforms...\")\n",
        "X_enhanced = enhance_features(X)\n",
        "print(f\"New Feature Count: {X_enhanced.shape[1]} (Original was {X.shape[1]})\")\n",
        "\n",
        "# Update X global variable\n",
        "X = X_enhanced"
      ],
      "metadata": {
        "id": "PS6lPyYdfLvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_simulation(model_class, param_grid, X, y, n_iters=20, scale=False, model_name=\"Model\"):\n",
        "    plcc_scores, srcc_scores, krcc_scores, rmse_scores = [], [], [], []\n",
        "\n",
        "    # Progress Bar Description\n",
        "    pbar = tqdm(range(n_iters), desc=f\"Running {model_name}\", leave=False)\n",
        "\n",
        "    for i in pbar:\n",
        "        # 1. Split Data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=None)\n",
        "\n",
        "        # 2. Scale (Only if needed)\n",
        "        if scale:\n",
        "            scaler = StandardScaler()\n",
        "            X_train = scaler.fit_transform(X_train)\n",
        "            X_test = scaler.transform(X_test)\n",
        "\n",
        "        # 3. Initialize & Fit\n",
        "        if isinstance(model_class, type) or callable(model_class):\n",
        "            if param_grid:\n",
        "                # Basic Grid Search\n",
        "                model = GridSearchCV(model_class(), param_grid, cv=3, n_jobs=-1, scoring='neg_mean_squared_error')\n",
        "                model.fit(X_train, y_train)\n",
        "                best_model = model.best_estimator_\n",
        "            else:\n",
        "                best_model = model_class()\n",
        "                best_model.fit(X_train, y_train)\n",
        "        else:\n",
        "            # It's an already instantiated object (like StackingRegressor)\n",
        "            best_model = model_class\n",
        "            best_model.fit(X_train, y_train)\n",
        "\n",
        "        # 4. Predict\n",
        "        preds = best_model.predict(X_test)\n",
        "\n",
        "        # 5. Metrics\n",
        "        plcc = pearsonr(y_test, preds)[0]\n",
        "        srcc = spearmanr(y_test, preds)[0]\n",
        "\n",
        "        plcc_scores.append(plcc)\n",
        "        srcc_scores.append(srcc)\n",
        "        krcc_scores.append(kendalltau(y_test, preds)[0])\n",
        "        rmse_scores.append(np.sqrt(mean_squared_error(y_test, preds)))\n",
        "\n",
        "        # Update progress bar with current PLCC/SRCC\n",
        "        pbar.set_postfix({'SRCC': f\"{srcc:.3f}\", 'PLCC': f\"{plcc:.3f}\"})\n",
        "\n",
        "    avg_metrics = [\n",
        "        np.mean(plcc_scores),\n",
        "        np.mean(srcc_scores),\n",
        "        np.mean(krcc_scores),\n",
        "        np.mean(rmse_scores)\n",
        "    ]\n",
        "    # Print final result after bar closes\n",
        "    print(f\"Finished {model_name} -> SRCC: {avg_metrics[1]:.4f} | PLCC: {avg_metrics[0]:.4f}\")\n",
        "    return avg_metrics"
      ],
      "metadata": {
        "id": "W1NpGBtYfTtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline for SVR to handle scaling automatically\n",
        "svr_pipeline = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    SVR(C=100, epsilon=0.1, gamma='scale')\n",
        ")\n",
        "\n",
        "models_config = {\n",
        "    \"SVR (RBF)\": {\n",
        "        \"class\": lambda: svr_pipeline,\n",
        "        \"params\": {},\n",
        "        \"scale\": False # Pipeline handles it\n",
        "    },\n",
        "    \"CatBoost\": {\n",
        "        \"class\": CatBoostRegressor,\n",
        "        \"params\": {'iterations': [800], 'learning_rate': [0.02], 'depth': [6], 'verbose': [0], 'allow_writing_files': [False]},\n",
        "        \"scale\": False\n",
        "    },\n",
        "    \"XGBoost\": {\n",
        "        \"class\": xgb.XGBRegressor,\n",
        "        \"params\": {'n_estimators': [500], 'learning_rate': [0.02], 'max_depth': [6], 'subsample': [0.7], 'n_jobs': [-1]},\n",
        "        \"scale\": False\n",
        "    },\n",
        "    \"LightGBM\": {\n",
        "        \"class\": lgb.LGBMRegressor,\n",
        "        \"params\": {'n_estimators': [500], 'learning_rate': [0.05], 'num_leaves': [31], 'verbose': [-1]},\n",
        "        \"scale\": False\n",
        "    },\n",
        "    \"Random Forest\": {\n",
        "        \"class\": RandomForestRegressor,\n",
        "        \"params\": {'n_estimators': [200]},\n",
        "        \"scale\": False\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "usX_FtsbfWyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ITERATIONS = 20 # Number of random splits per model\n",
        "\n",
        "all_results = []\n",
        "model_names = []\n",
        "\n",
        "print(f\"--- Starting Standard Benchmark ({ITERATIONS} iters) ---\\n\")\n",
        "\n",
        "for name, config in models_config.items():\n",
        "    metrics = run_simulation(\n",
        "        model_class=config[\"class\"],\n",
        "        param_grid=config[\"params\"],\n",
        "        X=X, y=y,\n",
        "        n_iters=ITERATIONS,\n",
        "        scale=config[\"scale\"],\n",
        "        model_name=name\n",
        "    )\n",
        "    all_results.append(metrics)\n",
        "    model_names.append(name)"
      ],
      "metadata": {
        "id": "chT8mP86fZly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Training Super-Stacking Ensemble ---\")\n",
        "\n",
        "# 1. Base Learners: A mix of Tree, Kernel, and Distance methods\n",
        "estimators = [\n",
        "    # Trees (Structure Experts)\n",
        "    ('cat', CatBoostRegressor(iterations=1000, learning_rate=0.02, depth=6, verbose=0, allow_writing_files=False)),\n",
        "    ('xgb', xgb.XGBRegressor(n_estimators=600, learning_rate=0.02, max_depth=6, subsample=0.7, n_jobs=-1)),\n",
        "\n",
        "    # Kernel (Math Expert) - Scaled internally\n",
        "    ('svr', make_pipeline(StandardScaler(), SVR(C=100, gamma='scale', epsilon=0.1))),\n",
        "\n",
        "    # Distance (Similarity Expert) - Scaled internally\n",
        "    # KNN helps fix outliers by looking at the 15 most similar images\n",
        "    ('knn', make_pipeline(StandardScaler(), KNeighborsRegressor(n_neighbors=15, weights='distance', metric='manhattan')))\n",
        "]\n",
        "\n",
        "# 2. Meta Learner: ElasticNetCV\n",
        "# Automatically tunes the mix of Ridge (L2) and Lasso (L1) regularization\n",
        "final_estimator = ElasticNetCV(\n",
        "    l1_ratio=[.1, .5, .7, .9, .95, .99],\n",
        "    cv=5,\n",
        "    max_iter=2000\n",
        ")\n",
        "\n",
        "# 3. Build Stack\n",
        "stacking_model = StackingRegressor(\n",
        "    estimators=estimators,\n",
        "    final_estimator=final_estimator,\n",
        "    n_jobs=-1,\n",
        "    passthrough=False\n",
        ")\n",
        "\n",
        "# 4. Run Simulation\n",
        "stack_metrics = run_simulation(\n",
        "    model_class=stacking_model,\n",
        "    param_grid={},\n",
        "    X=X, y=y,\n",
        "    n_iters=ITERATIONS,\n",
        "    scale=False, # Base models have pipelines\n",
        "    model_name=\"Super-Stack (KNN+Trees)\"\n",
        ")\n",
        "\n",
        "all_results.append(stack_metrics)\n",
        "model_names.append(\"Super-Stack (KNN+Trees)\")"
      ],
      "metadata": {
        "id": "yMoaluQPfdxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame\n",
        "df = pd.DataFrame(all_results, columns=['PLCC', 'SRCC', 'KRCC', 'RMSE'], index=model_names)\n",
        "\n",
        "df = df.sort_values(by='PLCC', ascending=False)\n",
        "\n",
        "# Print\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINAL LEADERBOARD\")\n",
        "print(\"=\"*50)\n",
        "print(df.round(4))\n",
        "\n",
        "# Plot Table\n",
        "fig, ax = plt.subplots(figsize=(12, len(df)*0.8 + 1))\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "\n",
        "# Color the \"Stacking\" row if it exists to highlight it\n",
        "cell_colours = [['#ffffff']*4 for _ in range(len(df))]\n",
        "for i, name in enumerate(df.index):\n",
        "    if \"Stack\" in name:\n",
        "        cell_colours[i] = ['#d1e7dd']*4 # Light Green for the winner\n",
        "\n",
        "table = ax.table(cellText=df.round(4).values,\n",
        "                 colLabels=df.columns,\n",
        "                 rowLabels=df.index,\n",
        "                 cellColours=cell_colours,\n",
        "                 cellLoc='center', loc='center')\n",
        "\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(12)\n",
        "table.scale(1.2, 1.5)\n",
        "plt.title(f\"Optimization Results (N={ITERATIONS})\", weight='bold', fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r0YPksxHffKd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
